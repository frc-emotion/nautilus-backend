name: "ci / staging"

on:
  push:
    branches:
      - staging
  workflow_dispatch:

jobs:
  deploy:
    runs-on: ubuntu-latest
    permissions:
      contents: read
      packages: write
      deployments: write

    env:
      DB_NAME: ${{ secrets.DB_NAME_STAGING }}
      MONGO_URI: ${{ secrets.MONGO_URI_STAGING }}
      ENVIRONMENT: ${{ secrets.ENVIRONMENT_STAGING }}
      EXPO_TOKEN: ${{ secrets.EXPO_TOKEN_STAGING }}
      JWT_SECRET: ${{ secrets.JWT_SECRET_STAGING }}
      MAILGUN_API_KEY: ${{ secrets.MAILGUN_API_KEY_STAGING }}
      MAILGUN_ENDPOINT: ${{ secrets.MAILGUN_ENDPOINT_STAGING }}
      MAILGUN_FROM_EMAIL: ${{ secrets.MAILGUN_FROM_EMAIL_STAGING }}
      DISCORD_WEBHOOK: ${{ secrets.DISCORD_WEBHOOK }}
      APP_MIGRATION_LEAD: ${{ secrets.APP_MIGRATION_LEAD_STAGING }}
      APP_MIGRATION_MEETING: ${{ secrets.APP_MIGRATION_MEETING_STAGING }}

    steps:
      - name: Checkout
        uses: actions/checkout@v3
        with:
          fetch-depth: 0

      - name: Set up QEMU
        uses: docker/setup-qemu-action@v3

      - name: Set up Docker Buildx
        uses: docker/setup-buildx-action@v3

      - name: Login to GHCR
        uses: docker/login-action@v3
        with:
          registry: ghcr.io
          username: ${{ github.actor }}
          password: ${{ secrets.GITHUB_TOKEN }}

      - name: Build and push container image
        uses: docker/build-push-action@v5
        with:
          context: .
          file: ./Dockerfile.staging
          platforms: linux/amd64
          push: true
          tags: ghcr.io/frc-emotion/staging-nautilus-backend:${{ github.sha }}

      - name: Install doctl
        uses: digitalocean/action-doctl@v2
        with:
          token: ${{ secrets.DIGITALOCEAN_ACCESS_TOKEN }}

      - name: Save DigitalOcean kubeconfig
        run: doctl kubernetes cluster kubeconfig save ${{ secrets.CLUSTER_NAME }}

      - name: Deploy to DigitalOcean Kubernetes
        run: |
          set -e
          
          echo "=== Starting deployment ==="
          
          # Create namespace if it doesn't exist
          kubectl create namespace staging --dry-run=client -o yaml | kubectl apply -f -
          
          # Create or update GHCR secret for pulling images
          kubectl create secret docker-registry ghcr-secret \
            --docker-server=ghcr.io \
            --docker-username=${{ github.actor }} \
            --docker-password=${{ secrets.GITHUB_TOKEN }} \
            --docker-email=${{ github.actor }}@users.noreply.github.com \
            --namespace=staging \
            --dry-run=client -o yaml | kubectl apply -f -
          
          # Calculate ConfigMap hash to force pod restart when config changes
          CONFIGMAP_HASH=$(echo -n "${{ env.MONGO_URI }}${{ env.JWT_SECRET }}${{ env.DB_NAME }}${{ env.EXPO_TOKEN }}" | sha256sum | cut -d' ' -f1)
          echo "ConfigMap hash: $CONFIGMAP_HASH"
          
          # Create configmap using YAML definition
          cat <<EOF | kubectl apply -f -
          apiVersion: v1
          kind: ConfigMap
          metadata:
            name: nautilus-config
            namespace: staging
          data:
            ENVIRONMENT: "${{ env.ENVIRONMENT }}"
            DB_NAME: "${{ env.DB_NAME }}"
            MONGO_URI: "${{ env.MONGO_URI }}"
            EXPO_TOKEN: "${{ env.EXPO_TOKEN }}"
            JWT_SECRET: "${{ env.JWT_SECRET }}"
            MAILGUN_API_KEY: "${{ env.MAILGUN_API_KEY }}"
            MAILGUN_ENDPOINT: "${{ env.MAILGUN_ENDPOINT }}"
            MAILGUN_FROM_EMAIL: "${{ env.MAILGUN_FROM_EMAIL }}"
            DISCORD_WEBHOOK: "${{ env.DISCORD_WEBHOOK }}"
            APP_MIGRATION_LEAD: "${{ env.APP_MIGRATION_LEAD }}"
            APP_MIGRATION_MEETING: "${{ env.APP_MIGRATION_MEETING }}"
          EOF

          # Export variables for envsubst
          export GITHUB_SHA=${{ github.sha }}
          export CONFIGMAP_HASH=$CONFIGMAP_HASH
          
          echo "=== Cleaning up stuck resources from previous failed deployments ==="
          
          # Get current deployment revision to identify old resources
          CURRENT_REVISION=$(kubectl get deployment nautilus-backend -n staging -o jsonpath='{.metadata.generation}' 2>/dev/null || echo "0")
          echo "Current deployment generation: $CURRENT_REVISION"
          
          # Clean up old failed replica sets (keep last 2)
          echo "Cleaning up old replica sets..."
          OLD_RS=$(kubectl get rs -n staging -l app=nautilus-backend --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
          if [ ! -z "$OLD_RS" ]; then
            RS_ARRAY=($OLD_RS)
            RS_COUNT=${#RS_ARRAY[@]}
            if [ $RS_COUNT -gt 2 ]; then
              # Delete all but the last 2 replica sets
              for ((i=0; i<$RS_COUNT-2; i++)); do
                echo "Scaling down old replica set: ${RS_ARRAY[$i]}"
                kubectl scale rs ${RS_ARRAY[$i]} -n staging --replicas=0 2>/dev/null || true
                kubectl delete rs ${RS_ARRAY[$i]} -n staging --ignore-not-found=true 2>/dev/null || true
              done
            fi
          fi
          
          # Force delete any pods that have been pending for more than 2 minutes (stuck from previous deployments)
          echo "Checking for long-stuck pending pods..."
          STUCK_PODS=$(kubectl get pods -n staging -l app=nautilus-backend -o json 2>/dev/null | \
            jq -r --argjson now $(date +%s) '.items[] | select(.status.phase == "Pending") | select(($now - (.metadata.creationTimestamp | fromdateiso8601)) > 120) | .metadata.name' 2>/dev/null || echo "")
          
          if [ ! -z "$STUCK_PODS" ]; then
            echo "Force deleting stuck pending pods (>2min old): $STUCK_PODS"
            for pod in $STUCK_PODS; do
              kubectl delete pod $pod -n staging --grace-period=0 --force 2>/dev/null || true
            done
            sleep 3
          fi
          
          # Force delete any terminating pods
          TERMINATING_PODS=$(kubectl get pods -n staging -l app=nautilus-backend --field-selector=status.phase=Terminating -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
          if [ ! -z "$TERMINATING_PODS" ]; then
            echo "Force deleting terminating pods: $TERMINATING_PODS"
            for pod in $TERMINATING_PODS; do
              kubectl delete pod $pod -n staging --grace-period=0 --force 2>/dev/null || true
            done
            sleep 3
          fi
          
          echo "=== Applying deployment manifests ==="
          envsubst '${GITHUB_SHA} ${CONFIGMAP_HASH}' < k8s/staging-manifests.yml | kubectl apply -f -
          
          # Wait a moment for Kubernetes to process the update
          sleep 5
          
          echo "=== Monitoring deployment progress ==="
          
          # Get the new replica set name
          NEW_RS=$(kubectl get rs -n staging -l app=nautilus-backend --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[-1].metadata.name}' 2>/dev/null || echo "")
          echo "New replica set: $NEW_RS"
          
          # Monitor for the first 60 seconds to catch early failures
          echo "Monitoring new pod startup..."
          for i in {1..12}; do
            sleep 5
            
            # Check if new pod exists and its status
            NEW_POD_STATUS=$(kubectl get pods -n staging -l app=nautilus-backend -o json 2>/dev/null | \
              jq -r --arg rs "$NEW_RS" '.items[] | select(.metadata.ownerReferences[0].name == $rs) | .status.phase' 2>/dev/null | head -1 || echo "")
            
            if [ "$NEW_POD_STATUS" = "Running" ]; then
              echo "New pod is running, checking readiness..."
              break
            elif [ "$NEW_POD_STATUS" = "Failed" ] || [ "$NEW_POD_STATUS" = "CrashLoopBackOff" ]; then
              echo "ERROR: New pod failed to start!"
              kubectl get pods -n staging -l app=nautilus-backend
              kubectl describe pods -n staging -l app=nautilus-backend
              kubectl logs -n staging -l app=nautilus-backend --tail=50 || true
              exit 1
            elif [ "$NEW_POD_STATUS" = "Pending" ]; then
              # Check if it's pending due to resource constraints
              PENDING_REASON=$(kubectl get pods -n staging -l app=nautilus-backend -o json 2>/dev/null | \
                jq -r --arg rs "$NEW_RS" '.items[] | select(.metadata.ownerReferences[0].name == $rs) | .status.conditions[] | select(.type == "PodScheduled" and .status == "False") | .reason' 2>/dev/null || echo "")
              
              if [ "$PENDING_REASON" = "Unschedulable" ]; then
                echo "WARNING: New pod is unschedulable (insufficient resources)"
                echo "Attempting to free resources by scaling down old replica set..."
                
                # Get old replica sets and scale them down
                OLD_RS_LIST=$(kubectl get rs -n staging -l app=nautilus-backend --sort-by=.metadata.creationTimestamp -o jsonpath='{.items[*].metadata.name}' 2>/dev/null || echo "")
                for rs in $OLD_RS_LIST; do
                  if [ "$rs" != "$NEW_RS" ]; then
                    echo "Scaling down old replica set: $rs"
                    kubectl scale rs $rs -n staging --replicas=0 2>/dev/null || true
                  fi
                done
                sleep 5
              fi
            fi
            
            echo "Waiting for pod to become ready... (attempt $i/12, status: $NEW_POD_STATUS)"
          done
          
          echo "=== Waiting for rollout to complete ==="
          # Use a longer timeout for the final rollout check
          if ! kubectl rollout status deployment/nautilus-backend -n staging --timeout=10m; then
            echo "ERROR: Deployment rollout failed!"
            echo ""
            echo "=== Deployment Status ==="
            kubectl get deployment nautilus-backend -n staging -o wide
            echo ""
            echo "=== Pod Status ==="
            kubectl get pods -n staging -l app=nautilus-backend -o wide
            echo ""
            echo "=== Recent Events ==="
            kubectl get events -n staging --sort-by='.lastTimestamp' | grep nautilus-backend | tail -20
            echo ""
            echo "=== Pod Descriptions ==="
            kubectl describe pods -n staging -l app=nautilus-backend
            echo ""
            echo "=== Pod Logs ==="
            kubectl logs -n staging -l app=nautilus-backend --tail=100 --all-containers=true || true
            exit 1
          fi
          
          echo "=== Deployment successful! ==="
          kubectl get pods -n staging -l app=nautilus-backend -o wide